{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD59REsWw-Om",
        "outputId": "a6f229b9-ab2a-4233-ea40-998cbb66c430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted email addresses: ['support@example.com', 'info@company.com']\n",
            "Email: support@example.com, Valid: True\n",
            "Email: info@company.com, Valid: True\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_emails(text):\n",
        "    # Define the regular expression pattern for matching email addresses\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "    # Use re.findall to find all matches in the text\n",
        "    matches = re.findall(email_pattern, text)\n",
        "\n",
        "    return matches\n",
        "\n",
        "def validate_email(email):\n",
        "    # Define the regular expression pattern for validating email addresses\n",
        "    email_pattern = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}$'\n",
        "\n",
        "    # Use re.match to check if the email address is valid\n",
        "    if re.match(email_pattern, email):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Please contact support@example.com for assistance or info@company.com for more information.\"\n",
        "\n",
        "# Extract and print email addresses from the text\n",
        "email_addresses = extract_emails(input_text)\n",
        "print(\"Extracted email addresses:\", email_addresses)\n",
        "\n",
        "# Validate and print whether each email address is valid\n",
        "for email in email_addresses:\n",
        "    is_valid = validate_email(email)\n",
        "    print(f\"Email: {email}, Valid: {is_valid}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_even(num):\n",
        "    return num % 2 == 0\n",
        "\n",
        "def finite_state_machine(input_string):\n",
        "    # Define states\n",
        "    states = {\n",
        "        'q0': {'0': 'q1', '1': 'q2'},\n",
        "        'q1': {'0': 'q3', '1': 'q0'},\n",
        "        'q2': {'0': 'q0', '1': 'q3'},\n",
        "        'q3': {'0': 'q2', '1': 'q1'}\n",
        "    }\n",
        "\n",
        "    # Initial state\n",
        "    current_state = 'q0'\n",
        "\n",
        "    # Process each symbol in the input string\n",
        "    for symbol in input_string:\n",
        "        if symbol not in states[current_state]:\n",
        "            # Invalid symbol, string is not accepted\n",
        "            return False\n",
        "        current_state = states[current_state][symbol]\n",
        "\n",
        "    # Check if the final state has an even count of 0's and 1's\n",
        "    return is_even(input_string.count('0')) and is_even(input_string.count('1'))\n",
        "\n",
        "# Example usage\n",
        "input_str1 = \"0011\"  # Accepted\n",
        "input_str2 = \"10101\"  # Accepted\n",
        "input_str3 = \"110\"    # Not accepted\n",
        "\n",
        "print(f\"{input_str1}: {finite_state_machine(input_str1)}\")\n",
        "print(f\"{input_str2}: {finite_state_machine(input_str2)}\")\n",
        "print(f\"{input_str3}: {finite_state_machine(input_str3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-etmf-8xC3G",
        "outputId": "0101a792-ab1c-405c-d2a7-e8e4febbd161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0011: True\n",
            "10101: False\n",
            "110: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def morphological_analysis(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # Perform part-of-speech tagging\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Unhappily she ran quickly\"\n",
        "\n",
        "# Perform morphological analysis\n",
        "analysis_result = morphological_analysis(sentence)\n",
        "\n",
        "# Print the result\n",
        "print(\"Morphological Analysis:\")\n",
        "for word, pos_tag in analysis_result:\n",
        "    print(f\"{word}: {pos_tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5iVGMRfxJIm",
        "outputId": "fe28eef4-94a1-4af0-9692-556a19a5d17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morphological Analysis:\n",
            "Unhappily: RB\n",
            "she: PRP\n",
            "ran: VBD\n",
            "quickly: RB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VerbStateMachine:\n",
        "    def __init__(self):\n",
        "        self.state = 'base'\n",
        "        self.past_tense = {\n",
        "            'walk': 'walked',\n",
        "            'jump': 'jumped'\n",
        "            # Add more verbs as needed\n",
        "        }\n",
        "\n",
        "    def generate_past_tense(self, verb):\n",
        "        return self.past_tense.get(verb, f\"{verb}ed\")\n",
        "\n",
        "    def parse_sentence(self, sentence):\n",
        "        words = sentence.split()\n",
        "        parsed_sentence = []\n",
        "\n",
        "        for word in words:\n",
        "            if self.state == 'base':\n",
        "                if word.lower() in self.past_tense:\n",
        "                    parsed_sentence.append(self.generate_past_tense(word.lower()))\n",
        "                else:\n",
        "                    parsed_sentence.append(word)\n",
        "            else:\n",
        "                parsed_sentence.append(word)\n",
        "\n",
        "        return ' '.join(parsed_sentence)\n",
        "\n",
        "# Example usage\n",
        "verb_fsm = VerbStateMachine()\n",
        "\n",
        "sentence1 = \"She walked to the park yesterday.\"\n",
        "sentence2 = \"He jumped over the fence.\"\n",
        "\n",
        "parsed_sentence1 = verb_fsm.parse_sentence(sentence1)\n",
        "parsed_sentence2 = verb_fsm.parse_sentence(sentence2)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "print(sentence1)\n",
        "print(sentence2)\n",
        "print(\"\\nParsed Sentences:\")\n",
        "print(parsed_sentence1)\n",
        "print(parsed_sentence2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzXzxz9WxMEg",
        "outputId": "dbe4dcc1-547c-4998-a9f2-e3d8b2b308fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentences:\n",
            "She walked to the park yesterday.\n",
            "He jumped over the fence.\n",
            "\n",
            "Parsed Sentences:\n",
            "She walked to the park yesterday.\n",
            "He jumped over the fence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def apply_porter_stemmer(words):\n",
        "    porter = PorterStemmer()\n",
        "    stemmed_words = [porter.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Example words\n",
        "input_words = [\"jumps\", \"jumping\", \"jumper\", \"easily\", \"running\", \"files\", \"flying\", \"files\"]\n",
        "\n",
        "# Apply Porter Stemmer\n",
        "output_words = apply_porter_stemmer(input_words)\n",
        "\n",
        "# Display results\n",
        "print(\"Original words:\")\n",
        "print(input_words)\n",
        "print(\"\\nAfter stemming:\")\n",
        "print(output_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHWv2cSpxUOO",
        "outputId": "f18f92c6-d1f9-4484-a8c0-687527986bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words:\n",
            "['jumps', 'jumping', 'jumper', 'easily', 'running', 'files', 'flying', 'files']\n",
            "\n",
            "After stemming:\n",
            "['jump', 'jump', 'jumper', 'easili', 'run', 'file', 'fli', 'file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download NLTK data for parts of speech tagging\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Perform parts of speech tagging\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Example texts\n",
        "text1 = \"The sun is shining brightly.\"\n",
        "text2 = \"I love reading interesting books.\"\n",
        "\n",
        "# Perform parts of speech tagging on each text\n",
        "pos_tags1 = pos_tagging(text1)\n",
        "pos_tags2 = pos_tagging(text2)\n",
        "\n",
        "# Display the results\n",
        "print(f\"POS tagging for '{text1}': {pos_tags1}\")\n",
        "print(f\"POS tagging for '{text2}': {pos_tags2}\")"
      ],
      "metadata": {
        "id": "uEYmHtuoxYBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f91a38-02de-43e6-8922-8dc5a217701f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging for 'The sun is shining brightly.': [('The', 'DT'), ('sun', 'NN'), ('is', 'VBZ'), ('shining', 'VBG'), ('brightly', 'RB'), ('.', '.')]\n",
            "POS tagging for 'I love reading interesting books.': [('I', 'PRP'), ('love', 'VBP'), ('reading', 'VBG'), ('interesting', 'VBG'), ('books', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLM93cvGBwy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}