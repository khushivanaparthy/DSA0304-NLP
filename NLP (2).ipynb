{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***1.Python program that demonstrate how to use regular expressions to validate and extract email addresses from a given text.***"
      ],
      "metadata": {
        "id": "aH4FG1JJDR_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD59REsWw-Om",
        "outputId": "b5812985-9124-44a7-a9c3-4916371e1bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted email addresses: ['support@example.com', 'info@company.com']\n",
            "Email: support@example.com, Valid: True\n",
            "Email: info@company.com, Valid: True\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_emails(text):\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    matches = re.findall(email_pattern, text)\n",
        "\n",
        "    return matches\n",
        "\n",
        "def validate_email(email):\n",
        "    email_pattern = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}$'\n",
        "    if re.match(email_pattern, email):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "input_text = \"Please contact support@example.com for assistance or info@company.com for more information.\"\n",
        "\n",
        "email_addresses = extract_emails(input_text)\n",
        "print(\"Extracted email addresses:\", email_addresses)\n",
        "\n",
        "for email in email_addresses:\n",
        "    is_valid = validate_email(email)\n",
        "    print(f\"Email: {email}, Valid: {is_valid}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2.Implement a python program that defines a finite state automation to recognize strings with an equal number of 0's and 1's***"
      ],
      "metadata": {
        "id": "ITd37fUNDwcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_even(num):\n",
        "    return num % 2 == 0\n",
        "\n",
        "def finite_state_machine(input_string):\n",
        "    states = {\n",
        "        'q0': {'0': 'q1', '1': 'q2'},\n",
        "        'q1': {'0': 'q3', '1': 'q0'},\n",
        "        'q2': {'0': 'q0', '1': 'q3'},\n",
        "        'q3': {'0': 'q2', '1': 'q1'}\n",
        "    }\n",
        "\n",
        "    current_state = 'q0'\n",
        "\n",
        "    for symbol in input_string:\n",
        "        if symbol not in states[current_state]:\n",
        "            return False\n",
        "        current_state = states[current_state][symbol]\n",
        "\n",
        "\n",
        "    return is_even(input_string.count('0')) and is_even(input_string.count('1'))\n",
        "\n",
        "input_str1 = \"0011\"  # Accepted\n",
        "input_str2 = \"10101\"  # Accepted\n",
        "input_str3 = \"110\"    # Not accepted\n",
        "\n",
        "print(f\"{input_str1}: {finite_state_machine(input_str1)}\")\n",
        "print(f\"{input_str2}: {finite_state_machine(input_str2)}\")\n",
        "print(f\"{input_str3}: {finite_state_machine(input_str3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-etmf-8xC3G",
        "outputId": "c43ba03b-d7bc-4c9f-cfa8-15479ff2a033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0011: True\n",
            "10101: False\n",
            "110: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3.Write a python program that uses the NLTK library to perform morphological analysis on sentence\"Unhappily ,she ran quickly\".***"
      ],
      "metadata": {
        "id": "-g_DWNJyGK5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def morphological_analysis(sentence):\n",
        "\n",
        "    words = word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    return pos_tags\n",
        "sentence = \"Unhappily she ran quickly\"\n",
        "\n",
        "analysis_result = morphological_analysis(sentence)\n",
        "\n",
        "print(\"Morphological Analysis:\")\n",
        "for word, pos_tag in analysis_result:\n",
        "    print(f\"{word}: {pos_tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5iVGMRfxJIm",
        "outputId": "bec5ca3a-df61-4f7f-c099-a62ff84710c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morphological Analysis:\n",
            "Unhappily: RB\n",
            "she: PRP\n",
            "ran: VBD\n",
            "quickly: RB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4.Implement a python program that creates a finite-state machine for parsing and generating the past tense forms of english verbs using the sentences\"She walked to the park yesterday\",\"He jumped over the fence\".***"
      ],
      "metadata": {
        "id": "stS6LkTeGdXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VerbStateMachine:\n",
        "    def __init__(self):\n",
        "        self.state = 'base'\n",
        "        self.past_tense = {\n",
        "            'walk': 'walked',\n",
        "            'jump': 'jumped'\n",
        "        }\n",
        "\n",
        "    def generate_past_tense(self, verb):\n",
        "        return self.past_tense.get(verb, f\"{verb}ed\")\n",
        "\n",
        "    def parse_sentence(self, sentence):\n",
        "        words = sentence.split()\n",
        "        parsed_sentence = []\n",
        "\n",
        "        for word in words:\n",
        "            if self.state == 'base':\n",
        "                if word.lower() in self.past_tense:\n",
        "                    parsed_sentence.append(self.generate_past_tense(word.lower()))\n",
        "                else:\n",
        "                    parsed_sentence.append(word)\n",
        "            else:\n",
        "                parsed_sentence.append(word)\n",
        "\n",
        "        return ' '.join(parsed_sentence)\n",
        "\n",
        "verb_fsm = VerbStateMachine()\n",
        "\n",
        "sentence1 = \"She walked to the park yesterday.\"\n",
        "sentence2 = \"He jumped over the fence.\"\n",
        "\n",
        "parsed_sentence1 = verb_fsm.parse_sentence(sentence1)\n",
        "parsed_sentence2 = verb_fsm.parse_sentence(sentence2)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "print(sentence1)\n",
        "print(sentence2)\n",
        "print(\"\\nParsed Sentences:\")\n",
        "print(parsed_sentence1)\n",
        "print(parsed_sentence2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzXzxz9WxMEg",
        "outputId": "5f316ca0-5708-4571-a5da-2bc5f18723f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentences:\n",
            "She walked to the park yesterday.\n",
            "He jumped over the fence.\n",
            "\n",
            "Parsed Sentences:\n",
            "She walked to the park yesterday.\n",
            "He jumped over the fence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***5.Develope a python program that applies the porter stemmer algorithm to a list of words,which stemming process and provide examples of words such as [\"jumps\",\"jumping\",\"jumper\",\"jumped\",\"easily\",\"running\",\"files\",\"flying\"]before and after stemming.***"
      ],
      "metadata": {
        "id": "bKp0crzPG5T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def apply_porter_stemmer(words):\n",
        "    porter = PorterStemmer()\n",
        "    stemmed_words = [porter.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "input_words = [\"jumps\", \"jumping\", \"jumper\", \"easily\", \"running\", \"files\", \"flying\", \"files\"]\n",
        "\n",
        "\n",
        "output_words = apply_porter_stemmer(input_words)\n",
        "\n",
        "print(\"Original words:\")\n",
        "print(input_words)\n",
        "print(\"\\nAfter stemming:\")\n",
        "print(output_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHWv2cSpxUOO",
        "outputId": "f0f2a3c4-571b-4ee8-996d-da957cf4e6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words:\n",
            "['jumps', 'jumping', 'jumper', 'easily', 'running', 'files', 'flying', 'files']\n",
            "\n",
            "After stemming:\n",
            "['jump', 'jump', 'jumper', 'easili', 'run', 'file', 'fli', 'file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***6.Write a python program that uses NLTK to perform parts-of-speech tagging on a given text\"The sun is shinning brightly\",\"I love reading intresting books\".***"
      ],
      "metadata": {
        "id": "O6C9p5YvHiDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(text):\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "\n",
        "text1 = \"The sun is shining brightly.\"\n",
        "text2 = \"I love reading interesting books.\"\n",
        "\n",
        "pos_tags1 = pos_tagging(text1)\n",
        "pos_tags2 = pos_tagging(text2)\n",
        "\n",
        "print(f\"POS tagging for '{text1}': {pos_tags1}\")\n",
        "print(f\"POS tagging for '{text2}': {pos_tags2}\")"
      ],
      "metadata": {
        "id": "uEYmHtuoxYBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b45c652-8608-4687-fadd-d2159de6a770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging for 'The sun is shining brightly.': [('The', 'DT'), ('sun', 'NN'), ('is', 'VBZ'), ('shining', 'VBG'), ('brightly', 'RB'), ('.', '.')]\n",
            "POS tagging for 'I love reading interesting books.': [('I', 'PRP'), ('love', 'VBP'), ('reading', 'VBG'), ('interesting', 'VBG'), ('books', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***7.Create a Python program that demonstrates stochastic parts-of-speech tagging for the given sentences\"The red car stopped at the traffic light\",\"She quickly ran to catch the bus\".***"
      ],
      "metadata": {
        "id": "VKL3VxmTTm9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzp6e4C_H-KW",
        "outputId": "e4aa292c-cf1f-4dbd-b539-d2d8d34ae2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.tag import hmm\n",
        "\n",
        "sentences = [\n",
        "    \"The red car stopped at the traffic light.\",\n",
        "    \"She quickly ran to catch the bus.\"\n",
        "]\n",
        "\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "tagged_sentences = [pos_tag(tokens) for tokens in tokenized_sentences]\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(tagged_sentences)\n",
        "\n",
        "new_sentences = [\n",
        "    \"A black cat crossed the street.\",\n",
        "    \"They were waiting for the train at the station.\"\n",
        "]\n",
        "\n",
        "for sentence in new_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged_tokens = tagger.tag(tokens)\n",
        "    print(tagged_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrNNQvS5IkeY",
        "outputId": "4f59baa5-0cb9-4b35-d2f4-5fb973983b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'DT'), ('black', 'DT'), ('cat', 'DT'), ('crossed', 'DT'), ('the', 'DT'), ('street', 'DT'), ('.', 'DT')]\n",
            "[('They', 'DT'), ('were', 'DT'), ('waiting', 'DT'), ('for', 'DT'), ('the', 'DT'), ('train', 'DT'), ('at', 'DT'), ('the', 'DT'), ('station', 'DT'), ('.', 'DT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***8.Implement a Python program that performs rule based parts-of-speech tagging using regular expressions using the given rules.***"
      ],
      "metadata": {
        "id": "GMxq_pJFJ1hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "patterns = [\n",
        "    (r'\\b(?:The|the)\\b', 'DET'),\n",
        "    (r'\\b(?:cat|dog)\\b', 'NOUN'),\n",
        "    (r'\\b(?:is|am|are)\\b', 'VERB'),\n",
        "    (r'\\b(?:quickly|brightly)\\b', 'ADV'),\n",
        "    (r'\\b(?:[A-Za-z]+)\\b', 'NOUN')\n",
        "]\n",
        "\n",
        "regexp_tagger = nltk.RegexpTagger(patterns)\n",
        "\n",
        "sentences = [\n",
        "    \"The cat is quick.\",\n",
        "    \"The dog is bright.\",\n",
        "    \"I am running quickly.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged_tokens = regexp_tagger.tag(tokens)\n",
        "    print(tagged_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgQMN2l0IomS",
        "outputId": "da18dabc-6b33-4b2b-deb9-54475e91e173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DET'), ('cat', 'NOUN'), ('is', 'VERB'), ('quick', 'NOUN'), ('.', None)]\n",
            "[('The', 'DET'), ('dog', 'NOUN'), ('is', 'VERB'), ('bright', 'NOUN'), ('.', None)]\n",
            "[('I', 'NOUN'), ('am', 'VERB'), ('running', 'NOUN'), ('quickly', 'ADV'), ('.', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***9.Develop a python program that uses a PCFG to parse a sentence using\"The cat chased the mouse\",the probabilities in square brackets indicate the likelihood of each rule need to apply.***"
      ],
      "metadata": {
        "id": "BnMX11xDKIf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ay2_WwWMAj7",
        "outputId": "ac58d9b1-797a-4055-e0f4-699c6cf93e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> Det N\n",
        "Det -> 'the'\n",
        "N -> 'cat' | 'mouse'\n",
        "VP -> V NP\n",
        "V -> 'chased'\n",
        "\"\"\")\n",
        "\n",
        "parser = nltk.ChartParser(grammar)\n",
        "sentence = [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"]\n",
        "\n",
        "for tree in parser.parse(sentence):\n",
        "    tree.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnswfmQuKHQs",
        "outputId": "f96ae45f-30e3-401b-8aaf-5e13b46d9a09"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              S                 \n",
            "      ________|_____             \n",
            "     |              VP          \n",
            "     |         _____|___         \n",
            "     NP       |         NP      \n",
            "  ___|___     |      ___|____    \n",
            "Det      N    V    Det       N  \n",
            " |       |    |     |        |   \n",
            "the     cat chased the     mouse\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***10.Write a python program that performs information retrival using the TF-IDF score.***"
      ],
      "metadata": {
        "id": "FkyHt8RFN8dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "docs = [\n",
        "    \"Natural language processing (NLP) is a field of study in artificial intelligence.\",\n",
        "    \"NLP techniques are used in various applications like machine translation and sentiment analysis.\",\n",
        "    \"The development of NLP tools and libraries has made text analysis easier.\"\n",
        "]\n",
        "\n",
        "query = \"What is natural language processing?\"\n",
        "\n",
        "all_texts = docs + [query]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
        "\n",
        "most_similar_index = cosine_similarities.argmax()\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Most Similar Document:\\n{docs[most_similar_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTEztmtGLnAj",
        "outputId": "04329900-7e85-4b57-f329-df991ea35d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is natural language processing?\n",
            "Most Similar Document:\n",
            "Natural language processing (NLP) is a field of study in artificial intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntufAmc-PSwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}